# -*- coding: utf-8 -*-
"""with_and_without_sentimentscore.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a-a-0rCFN6P97oHH-KlQRfUctXuOGbau

# 1. Data Loading and Preprocessing
"""

!pip install vaderSentiment

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# Load data from Yahoo Finance (Upload your CSV file)
from google.colab import drive
drive.mount('/content/drive')

# Read CSV file into DataFrame and set 'Date' as index
DJIA_path = '/content/drive/MyDrive/FINAL PROJECT/DJIA_table(train).csv'
djia_data = pd.read_csv(DJIA_path)

# Read CSV file into DataFrame and set 'Date' as index
reddit_path = '/content/drive/MyDrive/FINAL PROJECT/RedditNews(train).csv'
reddit_data = pd.read_csv(reddit_path)

# Initialize the VADER SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()

# Function to calculate sentiment polarity using VADER
def calculate_vader_sentiment(text):
    score = analyzer.polarity_scores(text)
    return round(score['compound'], 3)

# Calculate sentiment scores for Reddit news
reddit_data['SentimentScore'] = reddit_data['News'].apply(calculate_vader_sentiment)

# Aggregate sentiment scores by date
sentiment_by_date = reddit_data.groupby('Date')['SentimentScore'].mean().reset_index()

# Convert date columns to a consistent format
sentiment_by_date['Date'] = pd.to_datetime(sentiment_by_date['Date'])
djia_data['Date'] = pd.to_datetime(djia_data['Date'], dayfirst=True)

# Merge the datasets on the 'Date' column
merged_data = pd.merge(djia_data, sentiment_by_date, how='inner', on='Date')

# Select required columns
final_dataset = merged_data[['Date', 'Close', 'SentimentScore']]

# Sort the dataset by date in ascending order
final_dataset = final_dataset.sort_values(by='Date', ascending=True)

# Print a preview of the resulting dataset
print(final_dataset.head())

"""# 2. Data Normalization"""

# Normalize the 'Close' price using MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
final_dataset['NormalizedClose'] = scaler.fit_transform(final_dataset[['Close']])

# Display the scaled data
print(final_dataset[:5])

"""# 3. Creating Training and Testing Data"""

import numpy as np
from sklearn.preprocessing import MinMaxScaler

# Define look-back period (number of previous days to use as input)
look_back = 60  # Use 60 previous days to predict the next day

# Extract required columns for the dataset
data_with_sentiment = final_dataset[['NormalizedClose', 'SentimentScore']].values  # With Sentiment
data_without_sentiment = final_dataset[['NormalizedClose']].values  # Without Sentiment

# Normalize only the 'NormalizedClose' column
scaler = MinMaxScaler()
scaled_data_with_sentiment = scaler.fit_transform(data_with_sentiment)  # With Sentiment
scaled_data_without_sentiment = scaler.fit_transform(data_without_sentiment)  # Without Sentiment

# Split the data into training and testing sets (80% training, 20% testing)
train_size_with_sentiment = int(len(scaled_data_with_sentiment) * 0.8)
train_data_with_sentiment = scaled_data_with_sentiment[:train_size_with_sentiment]
test_data_with_sentiment = scaled_data_with_sentiment[train_size_with_sentiment - look_back:]

train_size_without_sentiment = int(len(scaled_data_without_sentiment) * 0.8)
train_data_without_sentiment = scaled_data_without_sentiment[:train_size_without_sentiment]
test_data_without_sentiment = scaled_data_without_sentiment[train_size_without_sentiment - look_back:]

# Function to create input-output datasets for LSTM
def create_dataset(data, look_back):
    X, y = [], []
    for i in range(look_back, len(data)):
        X.append(data[i - look_back:i, :])  # Append the sequence of features (look-back rows)
        y.append(data[i, 0])  # Predict the 'NormalizedClose' (index 0)
    return np.array(X), np.array(y)

# Create datasets with sentiment score
X_train_with, y_train_with = create_dataset(train_data_with_sentiment, look_back)
X_test_with, y_test_with = create_dataset(test_data_with_sentiment, look_back)

# Create datasets without sentiment score
X_train_without, y_train_without = create_dataset(train_data_without_sentiment, look_back)
X_test_without, y_test_without = create_dataset(test_data_without_sentiment, look_back)

# Reshape input data for LSTM
X_train_with = np.reshape(X_train_with, (X_train_with.shape[0], X_train_with.shape[1], X_train_with.shape[2]))
X_test_with = np.reshape(X_test_with, (X_test_with.shape[0], X_test_with.shape[1], X_test_with.shape[2]))

X_train_without = np.reshape(X_train_without, (X_train_without.shape[0], X_train_without.shape[1], 1))
X_test_without = np.reshape(X_test_without, (X_test_without.shape[0], X_test_without.shape[1], 1))

# Display the shape of the training and testing sets
print("Dataset with Sentiment:")
print("X_train_with shape:", X_train_with.shape)  # (samples, look_back, features)
print("y_train_with shape:", y_train_with.shape)  # (samples,)
print("X_test_with shape:", X_test_with.shape)    # (samples, look_back, features)
print("y_test_with shape:", y_test_with.shape)    # (samples,)

print("\nDataset without Sentiment:")
print("X_train_without shape:", X_train_without.shape)  # (samples, look_back, 1)
print("y_train_without shape:", y_train_without.shape)  # (samples,)
print("X_test_without shape:", X_test_without.shape)    # (samples, look_back, 1)
print("y_test_without shape:", y_test_without.shape)    # (samples,)

# Separate the original dataset into training and testing sets based on the split
train_data_len = len(X_train_with) + look_back  # Length of training data including look-back
train_dates = final_dataset['Date'][:train_data_len]  # Dates for the training set
test_dates = final_dataset['Date'][train_data_len:]   # Dates for the testing set

train = data_with_sentiment[:train_data_len]  # Training set
test = data_with_sentiment[train_data_len:]   # Testing set

# Plot the training and testing data with dates on the x-axis
plt.figure(figsize=(12, 6))
plt.plot(train_dates, train[:, 0], label='Training Data (Normalized Close)', color='blue')
plt.plot(test_dates, test[:, 0], label='Testing Data (Normalized Close)', color='orange')

# Format the x-axis for better readability (optional)
plt.gcf().autofmt_xdate()

# Add labels and title
plt.title("Stock Price - Training and Testing Data Split")
plt.xlabel("Date")
plt.ylabel("Stock Price")
plt.legend()
plt.show()

"""# 4. Building the LSTM Model"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout

# Model with Sentiment Score
print("Building LSTM model with sentiment score...")
model_with_sentiment = Sequential()

# Add LSTM and Dropout layers
model_with_sentiment.add(LSTM(50, return_sequences=True, input_shape=(X_train_with.shape[1], X_train_with.shape[2])))  # Input shape: (timesteps, features)
model_with_sentiment.add(Dropout(0.2))
model_with_sentiment.add(LSTM(50, return_sequences=False))
model_with_sentiment.add(Dropout(0.2))
model_with_sentiment.add(Dense(25))  # Dense layer with 25 neurons
model_with_sentiment.add(Dense(1))   # Output layer for the predicted 'Close' price

# Compile the model
model_with_sentiment.compile(optimizer='adam', loss='mean_squared_error')

# Display the model summary
model_with_sentiment.summary()

# Model without Sentiment Score
print("\nBuilding LSTM model without sentiment score...")
model_without_sentiment = Sequential()

# Add LSTM and Dropout layers
model_without_sentiment.add(LSTM(50, return_sequences=True, input_shape=(X_train_without.shape[1], 1)))  # Only one feature
model_without_sentiment.add(Dropout(0.2))
model_without_sentiment.add(LSTM(50, return_sequences=False))
model_without_sentiment.add(Dropout(0.2))
model_without_sentiment.add(Dense(25))  # Dense layer with 25 neurons
model_without_sentiment.add(Dense(1))   # Output layer for the predicted 'Close' price

# Compile the model
model_without_sentiment.compile(optimizer='adam', loss='mean_squared_error')

# Display the model summary
model_without_sentiment.summary()

# Train the LSTM model with sentiment score
print("Training the LSTM model with sentiment score...")
history_with_sentiment = model_with_sentiment.fit(
    X_train_with,
    y_train_with,
    batch_size=32,
    epochs=50,
    validation_split=0.2
)

# Train the LSTM model without sentiment score
print("\nTraining the LSTM model without sentiment score...")
history_without_sentiment = model_without_sentiment.fit(
    X_train_without,
    y_train_without,
    batch_size=32,
    epochs=50,
    validation_split=0.2
)

# Plot training and validation loss for both models
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))

# Plot loss for the model with sentiment score
plt.plot(history_with_sentiment.history['loss'], label='Training Loss (With Sentiment)', color='blue')
plt.plot(history_with_sentiment.history['val_loss'], label='Validation Loss (With Sentiment)', color='orange')

# Plot loss for the model without sentiment score
plt.plot(history_without_sentiment.history['loss'], label='Training Loss (Without Sentiment)', color='green')
plt.plot(history_without_sentiment.history['val_loss'], label='Validation Loss (Without Sentiment)', color='red')

# Add plot details
plt.title('Model Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""# 7. Plotting Predictions vs Actual Values"""

from sklearn.metrics import mean_squared_error, mean_absolute_error
from math import sqrt

# **Predictions for the Model with Sentiment Score**
print("\nEvaluating the model with sentiment score...")
train_predictions_with = model_with_sentiment.predict(X_train_with)
test_predictions_with = model_with_sentiment.predict(X_test_with)

# Inverse transform predictions and actual values
train_predictions_with_inv = scaler.inverse_transform(
    np.concatenate((train_predictions_with, np.zeros((train_predictions_with.shape[0], 1))), axis=1)
)[:, 0]

test_predictions_with_inv = scaler.inverse_transform(
    np.concatenate((test_predictions_with, np.zeros((test_predictions_with.shape[0], 1))), axis=1)
)[:, 0]

y_train_with_inv = scaler.inverse_transform(
    np.concatenate((y_train_with.reshape(-1, 1), np.zeros((y_train_with.shape[0], 1))), axis=1)
)[:, 0]

y_test_with_inv = scaler.inverse_transform(
    np.concatenate((y_test_with.reshape(-1, 1), np.zeros((y_test_with.shape[0], 1))), axis=1)
)[:, 0]

# Calculate RMSE and MAE for the model with sentiment score
train_rmse_with = sqrt(mean_squared_error(y_train_with_inv, train_predictions_with_inv))
test_rmse_with = sqrt(mean_squared_error(y_test_with_inv, test_predictions_with_inv))
train_mae_with = mean_absolute_error(y_train_with_inv, train_predictions_with_inv)
test_mae_with = mean_absolute_error(y_test_with_inv, test_predictions_with_inv)

print(f"Model with Sentiment - Train RMSE: {train_rmse_with}, Test RMSE: {test_rmse_with}")
print(f"Model with Sentiment - Train MAE: {train_mae_with}, Test MAE: {test_mae_with}")


# **Predictions for the Model without Sentiment Score**
print("\nEvaluating the model without sentiment score...")
train_predictions_without = model_without_sentiment.predict(X_train_without)
test_predictions_without = model_without_sentiment.predict(X_test_without)

# Inverse transform predictions and actual values
train_predictions_without_inv = scaler.inverse_transform(train_predictions_without)
test_predictions_without_inv = scaler.inverse_transform(test_predictions_without)
y_train_without_inv = scaler.inverse_transform(y_train_without.reshape(-1, 1))
y_test_without_inv = scaler.inverse_transform(y_test_without.reshape(-1, 1))

# Calculate RMSE and MAE for the model without sentiment score
train_rmse_without = sqrt(mean_squared_error(y_train_without_inv, train_predictions_without_inv))
test_rmse_without = sqrt(mean_squared_error(y_test_without_inv, test_predictions_without_inv))
train_mae_without = mean_absolute_error(y_train_without_inv, train_predictions_without_inv)
test_mae_without = mean_absolute_error(y_test_without_inv, test_predictions_without_inv)

print(f"Model without Sentiment - Train RMSE: {train_rmse_without}, Test RMSE: {test_rmse_without}")
print(f"Model without Sentiment - Train MAE: {train_mae_without}, Test MAE: {test_mae_without}")

"""# 7. Plotting Predictions vs Actual Values"""

import matplotlib.pyplot as plt

# Ensure the lengths of predictions and actual values match
assert len(test_predictions_with_inv) == len(y_test_with_inv), "Mismatch in length of test predictions (with sentiment) and actual values."
assert len(test_predictions_without_inv) == len(y_test_without_inv), "Mismatch in length of test predictions (without sentiment) and actual values."

# Define test dates for predictions with and without sentiment
test_dates_with = dates[-len(y_test_with_inv):]  # Test dates aligned with sentiment model
test_dates_without = dates[-len(y_test_without_inv):]  # Test dates aligned without sentiment model

# Plot the data
plt.figure(figsize=(14, 8))

# Plot actual prices (with sentiment)
plt.plot(
    test_dates_with,
    y_test_with_inv,
    label='Actual Prices (With Sentiment)', color='blue', linewidth=2
)

# Plot actual prices (without sentiment)
plt.plot(
    test_dates_without,
    y_test_without_inv,
    label='Actual Prices (Without Sentiment)', color='orange', linewidth=2
)

# Plot predictions (with sentiment)
plt.plot(
    test_dates_with,
    test_predictions_with_inv,
    label='Predictions (With Sentiment)', color='green', linestyle='--', linewidth=2
)

# Plot predictions (without sentiment)
plt.plot(
    test_dates_without,
    test_predictions_without_inv,
    label='Predictions (Without Sentiment)', color='red', linestyle='--', linewidth=2
)

# Add plot details
plt.title('Comparison of Predictions: With vs Without Sentiment Score')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.tight_layout()       # Adjust layout to fit all elements
plt.show()